{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suvaisnav/Video-Anomaly-Detection-Usig-Deep-Learning/blob/main/Video_Anomaly_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c81ojLYTlpM5"
      },
      "outputs": [],
      "source": [
        "!pip install pickle5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make use of google drive for taking input and storing output"
      ],
      "metadata": {
        "id": "pTC_-CnLetpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyg2QtOAluPX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RGB Feature extraction code using I3D"
      ],
      "metadata": {
        "id": "DaX7hMkReyt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjueLoAgl5Rd"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/v-iashin/video_features.git\n",
        "! pip install omegaconf==2.0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzZAb08xmAL9"
      },
      "outputs": [],
      "source": [
        "%cd video_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyk2cAw7mExl"
      },
      "outputs": [],
      "source": [
        "from models.i3d.extract_i3d import ExtractI3D\n",
        "from utils.utils import build_cfg_path, action_on_extraction\n",
        "from omegaconf import OmegaConf\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hfs0S7EJmHX9"
      },
      "outputs": [],
      "source": [
        "feature_type = 'i3d'\n",
        "folder_name='/content/drive/MyDrive/normal'\n",
        "args = OmegaConf.load(build_cfg_path(feature_type))\n",
        "args.video_paths = [''.join([folder_name, '/', i]) for i in os.listdir(folder_name)]\n",
        "args.extraction_fps = 32\n",
        "args.flow_type = 'raft' \n",
        "# Load the model\n",
        "extractor = ExtractI3D(args)\n",
        "model, class_head = extractor.load_model(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu2SjvyJoeT0"
      },
      "outputs": [],
      "source": [
        "import pickle5 as pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for video_path in args.video_paths:\n",
        "    print(f'Extracting for {video_path}') \n",
        "    features = extractor.extract(device, model, class_head, video_path)\n",
        "    ls.append({'feature':features['rgb'], 'label':0,'num':features['rgb'].shape[0]})\n",
        "with open('/content/drive/MyDrive/data/train/data_1.pkl', 'wb') as handle:\n",
        "  pickle.dump(ls, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "SivNS818ejXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing python torch version"
      ],
      "metadata": {
        "id": "FuJLP0roe8S6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baBPPccmpqjd"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model "
      ],
      "metadata": {
        "id": "uU01lguQfEHw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6ZI3mP6qYEd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(Attention, self).__init__()\n",
        "        self.conv = nn.Conv2d(1024, 256, 1)\n",
        "        self.gcn = GCN(opt, 256, 256)\n",
        "        self.fc = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1024, 1, 1)\n",
        "        x = self.conv(x)\n",
        "        # x = torch.tanh(x)\n",
        "        x = F.relu(x,inplace=False)\n",
        "        x = x.clone().view(-1, 256)\n",
        "        A = self.gcn(x)\n",
        "        x = self.fc(x)\n",
        "        mask = torch.sigmoid(x) + 1e-5\n",
        "        inverse_mask = torch.reciprocal(mask)\n",
        "        return mask, inverse_mask\n",
        "\n",
        "\n",
        "class Classification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classification, self).__init__()\n",
        "        self.fc = nn.Linear(1024, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.fc(x))\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(Network, self).__init__()\n",
        "        self.attention = Attention(opt)\n",
        "        self.classification = Classification()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        mask, inverse_mask = self.attention(x)\n",
        "        video_feature = torch.sum(x * mask, dim=0, keepdim=True) / torch.sum(mask)\n",
        "        video_score = self.classification(video_feature)\n",
        "        inverse_video_feature = torch.sum(x * inverse_mask, dim=0, keepdim=True) / torch.sum(inverse_mask)\n",
        "        inverse_video_score = self.classification(inverse_video_feature)\n",
        "        segments_scores = self.classification(x)\n",
        "        return video_score, inverse_video_score, mask, segments_scores\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, opt, in_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.opt = opt\n",
        "        if self.opt.C:\n",
        "            self.theta = nn.Linear(in_channels, in_channels)\n",
        "            self.phi = nn.Linear(in_channels, in_channels)\n",
        "        self.conv_d = nn.Linear(in_channels, out_channels)\n",
        "        if opt.residual:\n",
        "            self.down = nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        t, c = x.size()\n",
        "        A, M = self.generate_A(t, self.opt.width)\n",
        "        M = M.detach()\n",
        "        if self.opt.A:\n",
        "            A = A.detach()\n",
        "        else:\n",
        "            A = 0.\n",
        "        if self.opt.C:\n",
        "            theta = self.theta(x)\n",
        "            phi = self.phi(x)\n",
        "            C = torch.mm(theta, phi.permute(1, 0))\n",
        "            if self.opt.CM:\n",
        "                tmp = torch.exp(C - torch.max(C*M, dim=-1, keepdim=True)[0]) * M\n",
        "                A += tmp / tmp.sum(dim=-1, keepdim=True)\n",
        "            else:\n",
        "                A += F.softmax(C, dim=-1)\n",
        "        if self.opt.residual:\n",
        "            out = self.conv_d(torch.bmm(A, x.permute(0, 2, 1)).permute(0, 2, 1)) + self.down(x)\n",
        "        else:\n",
        "            out = self.conv_d(torch.mm(A, x))\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_A(dim, width=3):\n",
        "        A = torch.zeros(dim, dim, device='cuda', requires_grad=False)\n",
        "        min_value = -(width - 1) // 2\n",
        "        extent = [min_value+i for i in range(width)]\n",
        "        for i in range(dim):\n",
        "            for j in extent:\n",
        "                if i+j >=0 and i+j <=dim-1:\n",
        "                  A[i, i+j] = 1.\n",
        "        M = A\n",
        "        A = A/A.sum(dim=1, keepdim=True)\n",
        "        return A, M\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INPUT_DATA**\n"
      ],
      "metadata": {
        "id": "ttoZ3AS8fI34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjE1pgLsqkd1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle5 as pickle\n",
        "\n",
        "num_classes = 1\n",
        "\n",
        "\n",
        "class InputData(object):\n",
        "    def __init__(self, folder_name, shuffle=True):\n",
        "        \"\"\"\n",
        "        Note: Existing non data files in the folder will raise an exception\n",
        "        :param folder_name: The name of folder only including data files\n",
        "        :param shuffle: Whether shuffle data in each files or not\n",
        "        \"\"\"\n",
        "        #print(folder_name)\n",
        "        self.files_list = [''.join([folder_name, '/', i]) for i in os.listdir(folder_name)]\n",
        "        \n",
        "        self.num_file = len(self.files_list)\n",
        "        self.shuffle = shuffle\n",
        "        if shuffle:\n",
        "            self.order_files = random.sample(list(range(self.num_file)), self.num_file)\n",
        "            self.files_list = [self.files_list[i] for i in self.order_files]\n",
        "            \n",
        "        else:\n",
        "            self.order_files = list(range(self.num_file))\n",
        "        self.current_file_index = 0\n",
        "        self.current_video_index = 0\n",
        "        \n",
        "        \n",
        "        with open(self.files_list[0], 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "            # print(self.files_list[self.current_file_index])  ##\n",
        "        self.num_feature = len(self.data)\n",
        "        if shuffle:\n",
        "            self.order_feature = random.sample(list(range(self.num_feature)), self.num_feature)\n",
        "            self.data = [self.data[i] for i in self.order_feature]\n",
        "        else:\n",
        "            self.order_feature = list(range(self.num_feature))\n",
        "\n",
        "    def __check_index(self, size):\n",
        "        if self.current_video_index + size <= self.num_feature:\n",
        "            data = self.data[self.current_video_index: self.current_video_index+size]\n",
        "            self.current_video_index += size\n",
        "            return data\n",
        "        else:\n",
        "            num_excess = self.current_video_index + size - self.num_feature\n",
        "            data1 = self.data[self.current_video_index: self.num_feature]\n",
        "            self.current_file_index += 1\n",
        "            if self.current_file_index == self.num_file:\n",
        "                if self.shuffle:\n",
        "                    self.order_files = random.sample(list(range(self.num_file)), self.num_file)\n",
        "                    self.files_list = [self.files_list[i] for i in self.order_files]\n",
        "                else:\n",
        "                    self.order_files = list(range(self.num_file))\n",
        "                self.current_file_index = 0\n",
        "            with open(self.files_list[self.current_file_index], 'rb') as f:\n",
        "                self.data = pickle.load(f)\n",
        "            self.num_feature = len(self.data)\n",
        "            if self.shuffle:\n",
        "                self.order_feature = random.sample(list(range(self.num_feature)), self.num_feature)\n",
        "                self.data = [self.data[i] for i in self.order_feature]\n",
        "            else:\n",
        "                self.order_feature = list(range(self.num_feature))\n",
        "            data2 = self.data[0: num_excess]\n",
        "            self.current_video_index = num_excess\n",
        "            return data1 + data2\n",
        "    \n",
        "    def next_batch(self, size):\n",
        "        data = self.__check_index(size)\n",
        "        feature = []\n",
        "        labels = []\n",
        "        dims = []\n",
        "        for i in range(size):\n",
        "            # if data[i]['feature'].shape[0] > 400 and data[i]['feature'].ndim != 1:\n",
        "            #     feat = data[i]['feature'][0: 400, :]\n",
        "            #     feature.append(feat)\n",
        "            # else:\n",
        "            feature.append(data[i]['feature'])\n",
        "            \n",
        "            if data[i]['label'] == 0:\n",
        "                labels.append([0.])\n",
        "            else:\n",
        "                labels.append([1.])\n",
        "            dims.append(data[i]['num'])\n",
        "        return feature, np.array(labels, dtype=np.float32), np.array(dims, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAIN**                                                 \n",
        "train_feature_code_path is path of pickle file of train_data                                                      \n",
        "test_feature_code_path is path of pickle file of test_data\n"
      ],
      "metadata": {
        "id": "7bdTIXkLffVH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6sPhCzXqrsf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import csv\n",
        "import argparse\n",
        "\n",
        "import pdb\n",
        "\n",
        "train_feature_code_path = '/content/drive/MyDrive/t1'\n",
        "test_feature_code_path = '/content/drive/MyDrive/ttest'\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # input for training\n",
        "    parser.add_argument('--batch_size', default=10, type=int)\n",
        "    parser.add_argument('--iterations', default=10, type=int)\n",
        "    parser.add_argument('--epochs', default=100, type=int)\n",
        "    parser.add_argument('--lr', default=0.5e-3, type=float)\n",
        "    parser.add_argument('--restore', default=False, type=bool)\n",
        "    parser.add_argument('--sal_coe', default=0.5, type=float)\n",
        "    parser.add_argument('--weight_decay', default=0.2e-5, type=float)\n",
        "    parser.add_argument('--sal_ratio', default=0.3, type=float)\n",
        "    parser.add_argument('--save_path', default='/content/drive/MyDrive/train_out_3/checkpoints/', type=str)\n",
        "    parser.add_argument('--gpu_list', default=[0], type=list)\n",
        "    parser.add_argument('--TEST', default=True, type=bool)\n",
        "    parser.add_argument('--A', action='store_false')\n",
        "    parser.add_argument('--C', action='store_true')\n",
        "    parser.add_argument('--CM', action='store_false')\n",
        "    parser.add_argument('--residual', action='store_true')\n",
        "    parser.add_argument('--num_gcn', default=1, type=int)\n",
        "    parser.add_argument('--width', default=3, type=int)\n",
        "    args,unknown = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def tower_loss(net, features, labels, dims, args):\n",
        "    loss = []\n",
        "    inverse_loss = []\n",
        "    sum_sal_loss = []\n",
        "    labels = torch.from_numpy(labels).cuda()\n",
        "    for i in range(len(features)):\n",
        "        feature = torch.from_numpy(features[i]).cuda()\n",
        "        feature=feature.type(torch.cuda.FloatTensor) \n",
        "        video_score, inverse_video_score, mask, seg_scores = net(feature)\n",
        "        entropy_loss = F.binary_cross_entropy_with_logits(video_score, labels[i: i+1, :])\n",
        "        margin = torch.max(torch.tensor(0., device='cuda', requires_grad=False), (torch.sigmoid(seg_scores) - mask) ** 2 - args.sal_ratio ** 2)\n",
        "        count_nonzero = (margin != 0.).sum().detach().to(torch.float32)\n",
        "        sal_loss = torch.sum(margin) / (count_nonzero + 1e-6)\n",
        "        inverse_entropy_loss = labels[i, 0] * F.binary_cross_entropy_with_logits(inverse_video_score, torch.tensor([[0.]], requires_grad=False, device='cuda'))\n",
        "        loss.append(entropy_loss)\n",
        "        inverse_loss.append(inverse_entropy_loss + args.sal_coe * sal_loss)\n",
        "        m=sum(inverse_loss) / args.batch_size\n",
        "        sum_sal_loss.append(args.sal_coe * sal_loss)\n",
        "    return sum(loss) / args.batch_size,m , sum(sum_sal_loss) / args.batch_size\n",
        "\n",
        "\n",
        "def train():\n",
        "    args = parse_args()\n",
        "    print('Hyper-parameters:')\n",
        "    d_args = vars(args)\n",
        "    for i in d_args:\n",
        "        print('{}: {}'.format(i, d_args[i]))\n",
        "    gpu_list = args.gpu_list\n",
        "    num_gpus = len(gpu_list)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([str(i) for i in gpu_list])\n",
        "    net = Network(args)\n",
        "    net.to('cuda')\n",
        "    net.train()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    optimizer_ass = torch.optim.Adam(net.attention.parameters(), lr=args.lr)\n",
        "    train_data = InputData(train_feature_code_path, shuffle=True)\n",
        "    if not os.path.exists(args.save_path):\n",
        "        os.makedirs(args.save_path)\n",
        "    for i in range(args.epochs):\n",
        "        print('[*] Current epochs: %d ---' % i)\n",
        "        sum_loss = 0.\n",
        "        sum_inverse_loss = 0.0\n",
        "        sum_sum_sal_loss = 0.0\n",
        "        for j in range(args.iterations):\n",
        "            list_features, numpy_labels, numpy_dims = train_data.next_batch(size=args.batch_size)\n",
        "            loss, inverse_loss, sum_sal_loss = tower_loss(net, list_features, numpy_labels, numpy_dims, args)\n",
        "            optimizer.zero_grad()\n",
        "            optimizer_ass.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "            torch.autograd.set_detect_anomaly(True)\n",
        "            inverse_loss.backward()\n",
        "            optimizer_ass.step()\n",
        "            sum_loss += loss.item()\n",
        "            sum_inverse_loss += inverse_loss.item()\n",
        "            sum_sum_sal_loss += sum_sal_loss.item()\n",
        "        print('Loss: {:.3f}, Inverse Loss: {:.3f}, sal_loss: {:.3f}'.format(sum_loss / args.iterations, sum_inverse_loss / args.iterations, sum_sum_sal_loss / args.iterations))\n",
        "        if i > 30:\n",
        "            print(i)\n",
        "            torch.save(net.state_dict(), args.save_path + '{}.param'.format(i))\n",
        "    if args.TEST:\n",
        "        test(args)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Needed for test"
      ],
      "metadata": {
        "id": "-KIKsnIxfoeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq46eG7_trwU"
      },
      "outputs": [],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually creating groundtruth values "
      ],
      "metadata": {
        "id": "Agk60kqsiNf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGd_V8LMvJaU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "a=[0 for i in range(30)]\n",
        "a[2]=1\n",
        "a[3]=1\n",
        "a1=[0 for i in range(24)]\n",
        "a1[21]=a1[22]=a1[23]=1\n",
        "ra1=[]\n",
        "ra2=[]\n",
        "for i in range(len(a)):\n",
        "  for j in range(60):\n",
        "      ra1.append(a[i])\n",
        "\n",
        "for i in range(len(a1)):\n",
        "  for j in range(60):\n",
        "    ra2.append(a1[i])\n",
        "# b=np.array(ra1)\n",
        "b1=np.array(ra2)\n",
        "b2=[]\n",
        "# b2.append(b)\n",
        "b2.append(b1)\n",
        "print(b2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test "
      ],
      "metadata": {
        "id": "lU1UBFwTfs2J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VS49lS_uCiC"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "import numpy as np\n",
        "import pickle5 as pickle\n",
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import xlsxwriter\n",
        "import pdb\n",
        "from scipy.interpolate import interp1d\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # input for training\n",
        "    parser.add_argument('--batch_size', default=10, type=int)\n",
        "    parser.add_argument('--iterations', default=9, type=int)\n",
        "    parser.add_argument('--epochs', default=80, type=int)\n",
        "    parser.add_argument('--lr', default=0.5e-3, type=float)\n",
        "    parser.add_argument('--restore', default=False, type=bool)\n",
        "    parser.add_argument('--sal_coe', default=0.5, type=float)\n",
        "    parser.add_argument('--weight_decay', default=0.2e-5, type=float)\n",
        "    parser.add_argument('--sal_ratio', default=0.3, type=float)\n",
        "    parser.add_argument('--save_path', default='/content/drive/MyDrive/train_out_3/checkpoints/', type=str)\n",
        "    parser.add_argument('--gpu_list', default=[0], type=list)\n",
        "    parser.add_argument('--TEST', default=True, type=bool)\n",
        "\n",
        "    parser.add_argument('--A', action='store_false')\n",
        "    parser.add_argument('--B', action='store_false')\n",
        "    parser.add_argument('--C', action='store_true')\n",
        "    parser.add_argument('--BM', action='store_false')\n",
        "    parser.add_argument('--CM', action='store_false')\n",
        "    parser.add_argument('--residual', action='store_true')\n",
        "    parser.add_argument('--num_gcn', default=1, type=int)\n",
        "    parser.add_argument('--width', default=3, type=int)\n",
        "    args,unknown = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def test(args):\n",
        "    def draw_roc(tpr, fpr, auc):\n",
        "        plt.figure()\n",
        "        lw = 2\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.4f)' % auc)\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.0])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver operating characteristic example')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.savefig(\"./test.png\")\n",
        "        plt.cla()\n",
        "        plt.clf()\n",
        "        plt.close()\n",
        "\n",
        "    gts = np.load('/content/drive/MyDrive/mini/Contrastive-Attention-for-Video-Anomaly-Detection-main/gts.npy',allow_pickle=True)\n",
        "\n",
        "    test_features = test_data = InputData('/content/drive/MyDrive/ttest', shuffle=False)\n",
        "    net = Network(args).to('cuda')\n",
        "    net.eval()\n",
        "    best_auc = 0\n",
        "    best_epoch = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(31,args.epochs):\n",
        "            workbook = xlsxwriter.Workbook('./record.xlsx')\n",
        "            mask_sheet = workbook.add_worksheet('mask')\n",
        "            score_sheet = workbook.add_worksheet('score')\n",
        "            cell_format = workbook.add_format({'font_color': 'red'})\n",
        "            cell_format2 = workbook.add_format({'font_color': 'blue'})\n",
        "            print(net.load_state_dict(torch.load('/content/drive/MyDrive/train_out_3/checkpoints/' + '{}.param'.format(i))))\n",
        "            \n",
        "            pred = []\n",
        "            y = []\n",
        "            for j in range(2): \n",
        "                features = torch.from_numpy(test_features.next_batch(1)[0][0]).float().cuda()\n",
        "                video_scores, inverse_video_scores, masks, segments_scores = net(Variable(features))\n",
        "                row = np.squeeze(masks.cpu().numpy(), axis=1)\n",
        "                mask_sheet.write_row(j, 1, row.tolist())\n",
        "                mask_sheet.write(j, 0, np.mean(row), cell_format2)\n",
        "                mask_sheet.conditional_format(j, np.argmax(row)+1, j, np.argmax(row)+1, {'type': 'no_errors', 'format': cell_format})\n",
        "                row = np.squeeze(segments_scores.cpu().numpy(), axis=1)\n",
        "                score_sheet.write_row(j, 0, row.tolist())\n",
        "                score_sheet.conditional_format(j, np.argmax(row), j, np.argmax(row), {'type': 'no_errors', 'format': cell_format})\n",
        "                scores = np.squeeze(segments_scores.cpu().numpy())\n",
        "                video_score = video_scores.cpu().numpy()\n",
        "                if video_score[0, 0] < -2:\n",
        "                    scores += video_score[0, 0]\n",
        "                x = np.arange(0, scores.shape[0])\n",
        "                f = interp1d(x, scores, kind='linear', axis=0, fill_value='extrapolate')\n",
        "                scale_x = np.arange(0, scores.shape[0], 1 / 60)\n",
        "                pred += list(f(scale_x))\n",
        "                y += b2[j].tolist()\n",
        "            fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "            thresholds=list(map(lambda x:x-0.002,thresholds))\n",
        "            auc = metrics.auc(fpr, tpr)\n",
        "            draw_roc(tpr, fpr, auc)\n",
        "            print('Epoch: {}, AUC: {}'.format(i, auc))\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc\n",
        "                best_epoch = i\n",
        "            workbook.close()\n",
        "    print('Best_Epoch: {}, Best_AUC: {}'.format(best_epoch, best_auc))\n",
        "    return best_auc\n",
        "\n",
        "# Uncomment to perform only testing\n",
        "args = parse_args()\n",
        "print('Hyper-parameters:')\n",
        "d_args = vars(args)\n",
        "for i in d_args:\n",
        "  print('{}: {}'.format(i, d_args[i]))\n",
        "test(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Video_Anomaly_Detection.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}